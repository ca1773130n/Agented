---
phase: 05-observability-and-process-reliability
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/db/webhook_dedup.py
  - backend/app/db/schema.py
  - backend/app/db/migrations.py
  - backend/app/services/execution_service.py
  - backend/app/__init__.py
autonomous: true
verification_level: proxy

must_haves:
  truths:
    - "A webhook_dedup_keys table exists in SQLite with columns trigger_id TEXT, payload_hash TEXT, created_at REAL, and PRIMARY KEY (trigger_id, payload_hash)"
    - "An index idx_webhook_dedup_created exists on webhook_dedup_keys(created_at) for efficient TTL cleanup"
    - "Sending the same webhook payload twice within 10 seconds results in only one execution (second is deduplicated via DB INSERT OR IGNORE)"
    - "Restarting the server and sending the same payload a third time within the TTL window still deduplicates (key persisted in SQLite, survives restart)"
    - "Sending the same payload after the 10-second TTL expires results in a new execution (TTL cleanup allows re-delivery)"
    - "The in-memory _webhook_dedup dict and _webhook_dedup_lock are removed from ExecutionService"
    - "An APScheduler cleanup job runs every 60 seconds to delete expired dedup keys from the table"
  artifacts:
    - path: "backend/app/db/webhook_dedup.py"
      provides: "check_and_insert_dedup_key() and cleanup_expired_keys() functions"
      contains: "check_and_insert_dedup_key"
    - path: "backend/app/db/schema.py"
      provides: "webhook_dedup_keys table in fresh schema"
      contains: "webhook_dedup_keys"
    - path: "backend/app/db/migrations.py"
      provides: "Migration 55 adding webhook_dedup_keys table"
      contains: "webhook_dedup_keys"
    - path: "backend/app/services/execution_service.py"
      provides: "dispatch_webhook_event using DB-backed dedup instead of in-memory dict"
      contains: "check_and_insert_dedup_key"
  key_links:
    - from: "backend/app/services/execution_service.py"
      to: "backend/app/db/webhook_dedup.py"
      via: "dispatch_webhook_event calls check_and_insert_dedup_key()"
      pattern: "from.*webhook_dedup.*import.*check_and_insert_dedup_key"
    - from: "backend/app/__init__.py"
      to: "backend/app/db/webhook_dedup.py"
      via: "APScheduler job calls cleanup_expired_keys() every 60 seconds"
      pattern: "cleanup_expired_keys"
    - from: "backend/app/db/migrations.py"
      to: "backend/app/db/schema.py"
      via: "Migration 55 creates same table as fresh schema definition"
      pattern: "webhook_dedup_keys"
---

<objective>
Replace the in-memory webhook deduplication dict with a SQLite-backed dedup table using INSERT OR IGNORE for atomic check-and-insert, ensuring deduplication survives server restarts.

Purpose: The current in-memory `_webhook_dedup` dict in ExecutionService is lost on server restart, allowing duplicate webhook executions immediately after a restart. A DB-backed solution persists dedup keys across restarts. This satisfies OBS-03.
Output: webhook_dedup.py DB module, updated schema.py and migrations.py, refactored dispatch_webhook_event in execution_service.py, APScheduler cleanup job.
Research basis: 05-RESEARCH.md Recommendation 4 — SQLite INSERT OR IGNORE on UNIQUE constraint for atomic dedup, TTL cleanup via APScheduler, remove in-memory dict entirely.
</objective>

<execution_context>
@.planning/milestones/v0.1.0/phases/05-observability-and-process-reliability/05-RESEARCH.md
</execution_context>

<context>
@.planning/codebase/CONCERNS.md
@backend/app/db/connection.py
@backend/app/db/schema.py
@backend/app/db/migrations.py
@backend/app/services/execution_service.py
@backend/app/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create webhook_dedup_keys table in schema and migrations, and implement DB dedup functions</name>
  <files>backend/app/db/webhook_dedup.py, backend/app/db/schema.py, backend/app/db/migrations.py</files>
  <action>
    1. Create `backend/app/db/webhook_dedup.py` with two functions (based on 05-RESEARCH.md Pattern 4 and code example):

    a) `def check_and_insert_dedup_key(trigger_id: str, payload_hash: str, ttl_seconds: int = 10) -> bool`:
       - Takes trigger_id, payload_hash, and ttl_seconds as arguments
       - Uses `get_connection()` context manager from `app.db.connection`
       - First, DELETE any expired entry for this exact key pair: `DELETE FROM webhook_dedup_keys WHERE trigger_id = ? AND payload_hash = ? AND created_at < ?` with cutoff = `time.time() - ttl_seconds`
       - Then, INSERT OR IGNORE: `INSERT OR IGNORE INTO webhook_dedup_keys (trigger_id, payload_hash, created_at) VALUES (?, ?, ?)` with `time.time()`
       - `conn.commit()` after both operations
       - Return `cursor.rowcount > 0` — True means key was NEW (not a duplicate), False means duplicate exists
       - CRITICAL: Use INSERT OR IGNORE, NOT SELECT-then-INSERT. INSERT OR IGNORE is atomic and has no race condition between concurrent greenlets (05-RESEARCH.md Anti-Patterns).

    b) `def cleanup_expired_keys(ttl_seconds: int = 10) -> int`:
       - Deletes all rows where `created_at < (time.time() - ttl_seconds)`
       - Returns the count of deleted rows
       - Called by APScheduler every 60 seconds to prevent unbounded table growth

    2. In `backend/app/db/schema.py`, add the `webhook_dedup_keys` table definition to the `FRESH_SCHEMA` SQL string (append after the last existing CREATE TABLE):

    ```sql
    CREATE TABLE IF NOT EXISTS webhook_dedup_keys (
        trigger_id TEXT NOT NULL,
        payload_hash TEXT NOT NULL,
        created_at REAL NOT NULL,
        PRIMARY KEY (trigger_id, payload_hash)
    );
    CREATE INDEX IF NOT EXISTS idx_webhook_dedup_created
        ON webhook_dedup_keys (created_at);
    ```

    3. In `backend/app/db/migrations.py`:
       - Add a migration function `_migrate_v55_webhook_dedup_keys(conn)` that creates the table and index (same SQL as schema.py)
       - Append `(55, "webhook_dedup_keys", _migrate_v55_webhook_dedup_keys)` to the `VERSIONED_MIGRATIONS` list

    AVOID: Do NOT store the full webhook payload in the dedup table (only the hash is needed — 05-RESEARCH.md Anti-Patterns). Do NOT use SELECT-then-INSERT (race condition). Do NOT skip the created_at index (needed for efficient TTL cleanup).
  </action>
  <verify>
    1. cd backend && uv run python -c "from app.db.webhook_dedup import check_and_insert_dedup_key, cleanup_expired_keys; print('import OK')" (Level 1: Sanity)
    2. cd backend && uv run python -c "
from app.db.connection import get_connection
from app.database import init_db
init_db()
with get_connection() as conn:
    tables = [r[0] for r in conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall()]
    assert 'webhook_dedup_keys' in tables, f'Table missing. Found: {tables}'
    print('Table exists')
" (Level 1: Sanity)
    3. cd backend && uv run pytest (Level 1: Sanity — existing tests pass)
  </verify>
  <done>
    webhook_dedup_keys table created via migration 55. schema.py has the table for fresh installs. check_and_insert_dedup_key() returns True for new keys, False for duplicates. cleanup_expired_keys() removes expired entries.
  </done>
</task>

<task type="auto">
  <name>Task 2: Replace in-memory dedup with DB-backed dedup in ExecutionService and schedule cleanup</name>
  <files>backend/app/services/execution_service.py, backend/app/__init__.py</files>
  <action>
    1. In `backend/app/services/execution_service.py`:

    a) Remove the in-memory dedup infrastructure (lines ~103-108):
       - Delete `_webhook_dedup: Dict[str, float] = {}`
       - Delete `_webhook_dedup_lock = threading.Lock()`
       - Keep `WEBHOOK_DEDUP_WINDOW = 10` as a class constant (still used as TTL parameter)

    b) Add import at the top of the file:
       `from app.db.webhook_dedup import check_and_insert_dedup_key`

    c) In `dispatch_webhook_event()`, replace the in-memory dedup block (lines ~1153-1173) with DB-backed dedup:

       The existing code computes `payload_hash` using `hashlib.sha256(json.dumps(payload, sort_keys=True, default=str).encode()).hexdigest()[:16]` — KEEP this exact hash computation.

       Replace the `with cls._webhook_dedup_lock:` block with:
       ```python
       is_new = check_and_insert_dedup_key(
           trigger_id=trigger["id"],
           payload_hash=payload_hash,
           ttl_seconds=cls.WEBHOOK_DEDUP_WINDOW,
       )
       if not is_new:
           logger.info(
               "Webhook dedup: skipping duplicate dispatch for trigger '%s' (DB-backed)",
               trigger["name"],
           )
           continue
       ```

       Remove the `dedup_key` string construction (`f"{trigger['id']}:{payload_hash}"`) and the `now_epoch` timestamp variable (no longer needed — the DB function handles timestamps internally).

    d) If `threading` was only imported for the dedup lock, verify whether it is still needed for other locks/threads in the class. It IS still needed (rate limit lock, retry timers, daemon threads). Do NOT remove the threading import.

    2. In `backend/app/__init__.py`, inside `create_app()`, in the `if not testing:` block, add an APScheduler job for dedup key cleanup AFTER the existing scheduler jobs (after the stale conversation cleanup job):

    ```python
    # Schedule periodic webhook dedup key cleanup (every 60 seconds)
    from .db.webhook_dedup import cleanup_expired_keys

    if SchedulerService._scheduler:
        SchedulerService._scheduler.add_job(
            func=cleanup_expired_keys,
            trigger="interval",
            seconds=60,
            id="webhook_dedup_cleanup",
            replace_existing=True,
        )
    ```

    The 60-second interval is sufficient: with a 10-second TTL, entries are at most 70 seconds old after cleanup (05-RESEARCH.md Pitfall 4). The cleanup function is lightweight (single DELETE with indexed WHERE clause).

    AVOID: Do NOT keep the in-memory dict as a "fast-path cache" — the DB is the single source of truth (05-RESEARCH.md Open Question 2 recommends removing the dict entirely for simplicity). Do NOT change the hash computation (must remain compatible with existing behavior). Do NOT add a new import for `datetime` if not already present — the DB function uses `time.time()` internally.
  </action>
  <verify>
    1. cd backend && uv run pytest (Level 1: Sanity — all existing tests pass)
    2. Verify in-memory dedup is removed:
       cd backend && uv run python -c "from app.services.execution_service import ExecutionService; assert not hasattr(ExecutionService, '_webhook_dedup'), 'In-memory dict still exists'; print('In-memory dedup removed')" (Level 1: Sanity)
    3. Integration test (Level 2: Proxy):
       - Start the server
       - Send the same webhook payload twice via curl within 10 seconds
       - Verify only one execution was created (check execution logs endpoint)
       - Restart the server
       - Send the same payload a third time
       - Verify the third delivery is still deduplicated (key survived restart in DB)
       - Wait 15 seconds past the TTL window
       - Send the payload again
       - Verify this time it creates a new execution (TTL expired)
  </verify>
  <done>
    In-memory _webhook_dedup dict removed from ExecutionService. dispatch_webhook_event uses check_and_insert_dedup_key() for atomic DB-backed deduplication. APScheduler cleanup job runs every 60 seconds. Dedup survives server restarts. TTL expiry allows re-delivery after the window.
  </done>
</task>

</tasks>

<verification>
Level 1 (Sanity):
- `cd backend && uv run pytest` — all existing tests pass
- `webhook_dedup_keys` table exists in SQLite after server start
- In-memory `_webhook_dedup` dict no longer exists on ExecutionService class

Level 2 (Proxy):
- Send same webhook payload twice within 10 seconds — only one execution created
- Restart server, send payload a third time — third delivery is deduplicated (DB-backed, survives restart)
- Wait past TTL (15 seconds after last delivery), send again — new execution created (TTL expired)
- Verify cleanup job runs (check table row count after waiting > 60 seconds with no new webhooks)

Level 3 (Deferred):
- Load test with 100 rapid webhook deliveries — verify no race conditions, no duplicate executions
- Monitor webhook_dedup_keys table size under sustained traffic for 1 hour
</verification>

<success_criteria>
1. Sending the same webhook payload twice within 10 seconds results in only one execution
2. Restarting the server and sending the payload a third time still deduplicates (DB-backed persistence)
3. Sending after TTL expiry creates a new execution
4. In-memory _webhook_dedup dict is completely removed from ExecutionService
5. APScheduler cleanup job prevents unbounded table growth
6. cd backend && uv run pytest passes with zero failures
</success_criteria>

<output>
After completion, create `.planning/milestones/v0.1.0/phases/05-observability-and-process-reliability/05-03-SUMMARY.md`
</output>
